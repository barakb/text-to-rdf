# Ollama Local LLM Configuration
# Copy this to .env to use local Ollama models

# Setup:
# 1. Install Ollama: https://ollama.ai
# 2. Start Ollama: ollama serve
# 3. Pull model: ollama pull llama3.3:8b

# API key (use "ollama" for local models)
GENAI_API_KEY=ollama

# RDF Extraction Model: The LLM used for entity and relation extraction
# For Ollama, use just the model name (no "ollama:" prefix needed)
RDF_EXTRACTION_MODEL=qwen2.5:7b

# Temperature (0.0 = deterministic, 1.0 = creative)
GENAI_TEMPERATURE=0.3

# Max tokens in response
GENAI_MAX_TOKENS=4096

# Optional: Custom system prompt
# GENAI_SYSTEM_PROMPT="Extract RDF entities as JSON-LD..."

# Ontologies (comma-separated URLs)
# RDF_ONTOLOGIES=https://schema.org/,http://www.w3.org/2006/time#

# ============================================
# Entity Linking Configuration
# ============================================
# Enable entity linking to resolve names to canonical URIs (default: false)
ENTITY_LINKING_ENABLED=true

# Entity linking strategy: dbpedia, wikidata, none
ENTITY_LINKING_STRATEGY=dbpedia

# DBpedia Spotlight service URL
ENTITY_LINKING_SERVICE_URL=https://api.dbpedia-spotlight.org/en

# Minimum confidence threshold for entity linking (0.0-1.0)
ENTITY_LINKING_CONFIDENCE=0.5

# Other popular Ollama models you can try:
# RDF_EXTRACTION_MODEL=llama3.2      # Llama 3.2
# RDF_EXTRACTION_MODEL=mistral       # Mistral 7B
# RDF_EXTRACTION_MODEL=mixtral       # Mixtral 8x7B
# RDF_EXTRACTION_MODEL=phi3          # Phi-3
# RDF_EXTRACTION_MODEL=qwen2.5:7b    # Qwen 2.5 7B
